#overwrite example dictionary in flashBullet.py with this dictionary for data analysis flash cards
cards = {
    "Analytics": "The systematic computational analysis of data or statistics.",
    "Big Data": "Extremely large and complex data sets that require advanced techniques to process and analyze.",
    "Correlation": "A statistical measure that describes the relationship between two variables.",
    "Data Mining": "The process of discovering patterns or information from large datasets.",
    "Descriptive Statistics": "Summary statistics that describe the main features of a dataset.",
    "Hypothesis Testing": "A statistical method to test the validity of a claim or hypothesis about a population.",
    "Inferential Statistics": "The process of drawing conclusions about a population based on a sample of data.",
    "Machine Learning": "An artificial intelligence technique that enables computers to learn and make predictions without being explicitly programmed.",
    "Outlier": "An observation that significantly deviates from other observations in a dataset.",
    "Regression Analysis": "A statistical method for modeling the relationship between a dependent variable and one or more independent variables.",
    "Sampling": "The process of selecting a subset of individuals or items from a larger population.",
    "Standard Deviation": "A measure of the amount of variation or dispersion in a dataset.",
    "Time Series": "A sequence of data points collected and recorded at successive time intervals.",
    "Variable": "A characteristic or attribute that can take on different values in a dataset.",
    "ANOVA (Analysis of Variance)": "A statistical technique used to compare means between two or more groups.",
    "Bias": "A systematic error or deviation from the true value in data collection or analysis.",
    "Chi-Square Test": "A statistical test used to determine if there is a significant association between two categorical variables.",
    "Confidence Interval": "A range of values within which a population parameter is estimated to lie with a certain level of confidence.",
    "Cross-Validation": "A technique used to assess the performance and generalizability of a predictive model.",
    "Data Cleansing": "The process of identifying and correcting or removing errors, inconsistencies, and inaccuracies in a dataset.",
    "Decision Tree": "A predictive modeling tool that uses a tree-like structure to make decisions or predictions.",
    "Dimensionality Reduction": "The process of reducing the number of variables or dimensions in a dataset while preserving important information.",
    "Exploratory Data Analysis": "The process of analyzing and summarizing data to gain insights and identify patterns or relationships.",
    "Feature Engineering": "The process of creating new features or transforming existing features to improve the performance of a machine learning model.",
    "Hadoop": "An open-source framework for processing and storing large datasets in a distributed computing environment.",
    "Inference": "The process of drawing conclusions or making predictions about a population based on data from a sample.",
    "K-Means Clustering": "A popular unsupervised learning algorithm that groups similar data points into clusters.",
    "Logistic Regression": "A statistical model used to predict the probability of a binary or categorical outcome.",
    "Missing Data": "Data that is not available or not recorded for certain observations or variables in a dataset.",
    "Naive Bayes": "A probabilistic classifier based on Bayes' theorem with strong independence assumptions between features.",
    "Normal Distribution": "A symmetric bell-shaped probability distribution commonly used in statistical inference.",
    "Overfitting": "A modeling error that occurs when a statistical model is excessively complex and performs well on the training data but poorly on new, unseen data.",
    "P-value": "A measure of the strength of evidence against the null hypothesis in a statistical test.",
    "Principal Component Analysis (PCA)": "A technique for reducing the dimensionality of a dataset while preserving most of the variation in the data.",
    "Random Forest": "An ensemble learning method that combines multiple decision trees to make predictions or classifications.",
    "Resampling": "The process of repeatedly drawing samples from a dataset to estimate properties of the population.",
    "Sensitivity": "The ability of a statistical model to correctly identify positive cases.",
    "Specificity": "The ability of a statistical model to correctly identify negative cases.",
    "Supervised Learning": "A machine learning approach where models are trained using labeled data to make predictions or classifications.",
    "T-test": "A statistical test used to determine if there is a significant difference between the means of two groups.",
    "Unsupervised Learning": "A machine learning approach where models are trained using unlabeled data to find patterns or structures.",
    "Variance": "A measure of how spread out the values in a dataset are around the mean.",
    "Web Scraping": "The process of automatically extracting data from websites.",
    "Z-score": "A measure of how many standard deviations an observation or value is from the mean in a dataset.",
    "Association Rule Mining": "A data mining technique that discovers interesting relationships or associations between variables in large datasets.",
    "Cluster Analysis": "The process of grouping similar data points together based on their characteristics or properties.",
    "Data Visualization": "The graphical representation of data or information to aid understanding and interpretation.",
    "Ensemble Learning": "A machine learning technique that combines multiple models to improve performance and robustness.",
    "Feature Selection": "The process of selecting a subset of relevant features from a larger set of variables to improve model performance.",
    "Gini Index": "A measure of impurity or node purity used in decision tree algorithms for determining the quality of a split.",
    "Hierarchical Clustering": "A clustering algorithm that organizes data points into a tree-like structure of nested clusters.",
    "Log-Likelihood": "A measure of how well a statistical model fits observed data.",
    "Multicollinearity": "The presence of strong correlations between independent variables in a regression model.",
    "Multivariate Analysis": "Statistical techniques used to analyze data with multiple variables or attributes.",
    "Nonparametric Test": "A statistical test that does not require assumptions about the underlying distribution of the data.",
    "Null Hypothesis": "A statement of no effect or no difference in a statistical test.",
    "One-Hot Encoding": "A technique used to convert categorical variables into binary vectors for machine learning models.",
    "Overfitting": "A modeling error that occurs when a statistical model is overly complex and performs poorly on new, unseen data.",
    "Precision": "The proportion of true positive predictions out of all positive predictions made by a model.",
    "Principal Component": "A linear combination of variables that captures most of the variation in a dataset.",
    "Python": "A popular programming language widely used for data analysis and machine learning tasks.",
    "Recall": "The proportion of true positive predictions out of all actual positive cases in a dataset.",
    "Root Mean Square Error (RMSE)": "A measure of the average difference between predicted and actual values in a regression model.",
    "Singular Value Decomposition (SVD)": "A matrix factorization method used for dimensionality reduction and data compression.",
    "Statistical Power": "The probability of correctly rejecting the null hypothesis when it is false.",
    "Support Vector Machine (SVM)": "A machine learning algorithm used for classification and regression tasks.",
    "Type I Error": "Rejecting the null hypothesis when it is true (false positive).",
    "TypeII Error": "Failing to reject the null hypothesis when it is false (false negative).",
    "Validation Set": "An independent dataset used to evaluate the performance of a machine learning model.",
    "VIF (Variance Inflation Factor)": "A measure of multicollinearity in a regression model.",
    "Wilcoxon Rank-Sum Test": "A nonparametric test used to compare the distributions of two independent samples.",
    "Zero-Inflated Model": "A statistical model used to analyze data with excess zeros compared to a standard distribution.",
    "A/B Testing": "A controlled experiment used to compare the effectiveness of two different options or treatments.",
    "Bagging": "An ensemble learning technique that combines multiple models trained on different subsets of the data.",
    "Bayesian Statistics": "A branch of statistics that incorporates prior knowledge or beliefs to make inferences or predictions.",
    "Bias-Variance Tradeoff": "The relationship between the flexibility and complexity of a model and its ability to generalize to new data.",
    "Box Plot": "A graphical representation of the distribution of a dataset through quartiles and outliers.",
    "Causality": "The relationship between cause and effect, often studied using experimental designs.",
    "Confusion Matrix": "A table that shows the performance of a classification model by comparing predicted and actual values.",
    "Cross-Entropy Loss": "A loss function used in machine learning models that outputs probabilities.",
    "Dask": "A parallel computing library in Python for processing large datasets.",
    "Data Wrangling": "The process of cleaning, transforming, and preparing raw data for analysis.",
    "Deep Learning": "A subfield of machine learning that focuses on artificial neural networks with multiple layers.",
    "Elastic Net": "A regularization method that combines L1 (Lasso) and L2 (Ridge) penalties.",
    "Feature Importance": "A technique to determine the relevance or importance of different features in a machine learning model.",
    "Grid Search": "A method to systematically tune hyperparameters of a machine learning model by trying all possible combinations.",
    "Imputation": "The process of replacing missing or null values in a dataset with estimated or imputed values.",
    "K-Nearest Neighbors (KNN)": "A non-parametric classification algorithm that assigns a data point to the majority class of its k nearest neighbors.",
    "Lift": "A measure of the effectiveness of a predictive model at identifying cases of interest compared to random chance.",
    "L1 Regularization (Lasso)": "A regularization technique that adds a penalty term equal to the absolute value of the coefficients.",
    "L2 Regularization (Ridge)": "A regularization technique that adds a penalty term equal to the squared value of the coefficients.",
    "Multilayer Perceptron (MLP)": "A type of feedforward artificial neural network with multiple layers of nodes.",
    "Naive Bayes Classifier": "A simple probabilistic classifier based on Bayes' theorem with strong independence assumptions between features.",
    "Neural Network": "A computational model inspired by the structure and function of biological neural networks.",
    "Over-sampling": "A technique to address class imbalance by increasing the number of instances in the minority class.",
    "PCA (Principal Component Analysis)": "A dimensionality reduction technique that transforms a dataset into a new set of variables called principal components.",
    "Precision-Recall Curve": "A graphical representation of the tradeoff between precision and recall for different classification thresholds.",
    "Recurrent Neural Network (RNN)": "A type of neural network designed to process sequential data, where information can be passed from previous steps to future steps.",
    "Regularization": "A technique used to prevent overfitting in a model by adding a penalty term to the loss function.",
    "ROC Curve (Receiver Operating Characteristic Curve)": "A graphical representation of the tradeoff between true positive rate and false positive rate for different classification thresholds.",
    "Spearman's Rank Correlation": "A nonparametric measure of the strength and direction of the monotonic relationship between two variables.",
    "Time Series Analysis": "A statistical analysis technique used to understand and model data that varies over time.",
    "Under-sampling": "A technique to address class imbalance by decreasing the number of instances in the majority class.",
    "XGBoost": "An optimized gradient boosting framework used for supervised learning tasks.",
}
